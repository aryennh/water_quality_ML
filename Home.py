# -*- coding: utf-8 -*-
"""water_potability-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15i5BHfc2bhmHGhMLSU-j6BMNfUVYpMKs
"""

# from google.colab import drive
# drive.mount('/content/drive')

#Data Pre-processing and Visualisation
import numpy as np
import pandas as pd
import streamlit as st
# import seaborn as sns
import panel as pn
import hvplot.pandas
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
import streamlit
from scipy.stats import probplot
from scipy.stats import zscore
from sklearn.impute import SimpleImputer

#Data Transformation
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from datetime import time

# st.sidebar.
# appointment = st.sidebar.slider(
#     "Schedule your appointment:",
#     value=(time(11, 30), time(12, 45)))

# pn.extension(design ='material')
#
# def view_hvplot(avg, highlight):
#     return avg.hvplot(height=300, width=400, legend=False) * highlight.hvplot.scatter(
#         color="orange", padding=0.1, legend=False
#     )
#
# def find_outliers(data,variable,window=30, sigma=10, view_fn=view_hvplot):
#     avg = data[variable].rolling(window=window).mean()
#     residual = data[variable] - avg
#     std = residual.rolling(window=window).std()
#     outliers = np.abs(residual) > std * sigma
#     return view_fn(avg, avg[outliers])

#Data Modelling
import keras
import sklearn
from sklearn import svm
from sklearn.svm import SVC
import tensorflow as tf
from keras.models import Sequential
# from tensorflow.keras.models import Model
from keras.layers import Dense, Embedding, LSTM, Dropout, Activation


#Model Evaluation
from scipy import stats
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

"""# ** Water Quality Index Prediction 
"""
# from bokeh.sampledata.autompg import autompg

# Importing the dataset
df = pd.read_csv("water_dataX(1).csv", encoding= 'unicode_escape',)
df.shape

# df.head()

# Checking for datatypes of the dataset
# df.dtypes

# Changing column names
df = df.rename(columns={"LOCATIONS": "Address", "STATE": "Location", "D.O. (mg/l)": "DO", "CONDUCTIVITY (µmhos/cm)": "Conductivity",
                        "B.O.D. (mg/l)": "BOD", "NITRATENAN N+ NITRITENANN (mg/l)": "NI",
                        "FECAL COLIFORM (MPN/100ml)": "Fec_col", "TOTAL COLIFORM (MPN/100ml)Mean": "Tot_col"})

# df.describe()
# find_outliers(df,"LOCATIONS", window=20, sigma=10)

"""# **Data Cleaning/Pre-processing**"""

# Converting object data type kto numeric
def convert_to_numeric(df):
    num_col = df.shape[1]
    # Start from index 3
    for index in range(3, num_col):
        col_name = df.iloc[:, index].name
        df[col_name] = pd.to_numeric(df[col_name], errors="coerce")
    return df

df = convert_to_numeric(df)
# df.dtypes

# df.shape[1]

# Checking for missing values
# df.isnull().sum().sort_values()

# Replacing string NAN values with actual NAN value (np.nan)
def convert_to_nan(df):
    n_col = df.shape[1]
    for index in range(n_col):
        df.iloc[:, index]  = df.iloc[:, index].replace("NAN", np.nan)
    return df

df = convert_to_nan(df)

# Checking for missing values
df.isnull().sum().sort_values()

# Replacing NULL values with median of column in numeric columns
df_num = df.select_dtypes(exclude="object")
df_num_col = df_num.columns
imputer = SimpleImputer(strategy="median")

df_num = imputer.fit_transform(df_num)
df_num = pd.DataFrame(df_num, columns=df_num_col)

# Checking for missing values in categorical columns
df_cat = df.select_dtypes(include="object")
# df_cat.isnull().sum()

# Here we can fill these values by obeserving other attributes
# Example -
pd.set_option('mode.chained_assignment', None)
df_cat_copy = df_cat.copy()

# STATION CODE with value 1330 belong in LOCATION - TAMIL NADU
# df_cat_copy[df_cat_copy["STATION CODE"] == "1330"]

# I can replace all the NAN occurences in LOCATION with TAMILNADU
df_cat_copy["Location"][df_cat_copy["STATION CODE"] == "1330"] = df_cat_copy["Location"][df_cat_copy["STATION CODE"] == "1330"].fillna("TAMILNADU")

# df_cat_copy[df_cat_copy["STATION CODE"] == "1330"]

# The NULL values in ADDRESS attribute are in LOCATION attribute (ex - above output)
# So, I will fill the NULL values in ADDRESS attribute with the corresponding LOCATION value

def fill_address(df_cat):
    address_null = df_cat[df_cat["Address"].isnull()]
    address_null_indices = address_null.index
    for index in address_null_indices:
        location_value = address_null["Location"][index]
        address_null["Address"][index] = location_value
        address_null["Location"][index] = np.nan
    df_cat[df_cat["Address"].isnull()] = address_null
    return

fill_address(df_cat_copy)
# df_cat_copy[df_cat_copy["STATION CODE"] == "1330"]

# df_cat_copy[df_cat_copy["STATION CODE"] == "17"]

# df_cat_copy[df_cat_copy["Address"] == "TAMBIRAPARANI AT ARUMUGANERI, TAMILNADU"]

# Now I can fill NULL values of STATION CODE with corresponding ADDRESS value(ex - above dataframe)
# I will match the corresponding similar ADDRESS values to fill in the STATION CODE

def fill_code(df_cat):
    station_null = df_cat[df_cat["STATION CODE"].isnull()]
    station_null_indices = station_null.index
    for index in station_null_indices:
        stat_code = np.nan
        address_index = station_null["Address"][index]
        code_at_address = df_cat["STATION CODE"][df_cat["Address"] == address_index]
        for index_code in code_at_address.index:
            if (code_at_address[index_code] != np.nan):
                stat_code = code_at_address[index_code]
                break
        station_null["STATION CODE"][index] = stat_code
    df_cat[df_cat["STATION CODE"].isnull()] = station_null
    return

fill_code(df_cat_copy)
# df_cat_copy[df_cat_copy["Address"] == "TAMBIRAPARANI AT ARUMUGANERI, TAMILNADU"]

# df_cat_copy[df_cat_copy["STATION CODE"] == "17"]

# Filling all LOCATION NAN values which have corresponding STATION CODE values
def fill_location(df_cat):
    station_code = df_cat["STATION CODE"].unique()
    for index in range(station_code.shape[0]):
        if (station_code[index] != np.nan):
            df_location = df_cat["Location"][df_cat["STATION CODE"] == station_code[index]]
            location_values = df_cat["Location"][df_cat["STATION CODE"] == station_code[index]]
            location = np.nan
            for index_location in range(location_values.shape[0]):
                if (location_values.iloc[index_location] != np.nan):
                    location = location_values.iloc[index_location]
                    break
            df_location_fill = df_location.fillna(location)
            df_cat["Location"][df_cat["STATION CODE"] == station_code[index]] = df_location_fill
    return


fill_location(df_cat_copy)
# df_cat_copy[df_cat_copy["STATION CODE"] == "1330"]

# Checking to see if other previously NULL values were also filled up
# df_cat_copy[df_cat_copy["STATION CODE"] == "17"]

# Checking NULL values after imputation
df_cat_copy.isnull().sum()
# '''
# df_cat_copy[df_cat_copy["Location"].isnull()]
#
# df_cat_copy[df_cat_copy["STATION CODE"] == "1207"]
#
# df_cat_copy[df_cat_copy["STATION CODE"] == "1208"]
#
# df_cat_copy[df_cat_copy["STATION CODE"] == "2047"]
#
# df_cat_copy[df_cat_copy["STATION CODE"] == "2048"]
#
# df_cat_copy[df_cat_copy["STATION CODE"] == "2049"]
# '''

# Filling up LOCATION based on ADDRESS
df_cat_copy["Location"][1649] = "CHANDIGARH"
df_cat_copy["Location"][1650] = "CHANDIGARH"
df_cat_copy["Location"][1769] = "CHANDIGARH"
df_cat_copy["Location"][1770] = "CHANDIGARH"
df_cat_copy["Location"][2047] = "CHANDIGARH"

# Re-checking NULL values
df_cat = df_cat_copy
df_cat.isnull().sum()

# df_cat_copy[df_cat_copy["Location"].isnull()]

df_num.isnull().sum()

# Combining categorical and numerical dataframes into one after pre-processing and checking for NULL total
df_final = pd.concat([df_cat, df_num], axis=1)
# df_final.isnull().sum()

# These are the samples which don't contain any attribute that I can use to fill in the NULL values
# So it is best to remove them

df_null = df_final[(df_final["STATION CODE"].isnull()) | (df_final["Location"].isnull())]
df_null_indices = df_null.index
df_final.drop(df_null_indices, axis=0, inplace=True)
# df_null

# Final checking of NULL values
# df_final.isnull().sum()

# Checking dataframe shape after removing NULL values
# df_final.shape

# df_final.describe()
# s
# PLotting   the numeric attributes in the dataset

df_num_final = df_final.select_dtypes(exclude="object")

# with st._container_transformer_manager(st.echo):
def plot_kde(df):
    n_col = df.shape[1]
    for index in range(n_col):
        col_index = df.iloc[:, index]
        fig, ax = plt.subplots(1,1, figsize=(7, 5))
        # appointment = st.sidebar.slider(
        #     "adjust values",
        #     value=(fig,ax))

        sns.kdeplot(data=df, x=col_index.name)
        st.pyplot(fig)

plot_kde(df_num_final)


# Removing outliers using z-score
z = np.abs(zscore(df_num_final))
print(z)

# Adding the z_score for each row into the dataframe
df_final['z_score'] = z.values.tolist()

# df_final

df_num_final_o = df_final[(z < 3).all(axis=1)]

df_num_final_o.drop(['z_score'], axis=1, inplace=True)

# df_num_final_o.shape

df_final = df_num_final_o

# df_final.shape

# KDE plots after removal of outliers
# st.sidebar.<widget>
a = st.sidebar.radio('R:',[1,2])
plot_kde(df_final.select_dtypes(exclude="object"))

"""# **Water Quality Index Calculation**

I will be using "Weighted Arithmetic Water Quality Index Method" to calculate WQI of each water sample. The formula to calculate WQI is -

> *WQI = ∑ qi × wi*

qi - Quality estimate scale of each parameter, it is calculated with the formula -
> *qi = 100 × ( Vi / Si)*

wi is calculated by the formula -
> *wi = K / Si*


Here K is proportionality constant which is -
> *K = 1 / ∑ (1/Si)*

### **Unit weights for each parameter (wi) -**
Dissolved Oxygen =  0.2213

pH = 0.2604

Conductivity = 0.0022

Biological Oxygen Demand = 0.4426

Nitrate = 0.0492

Fecal Coliform = 0.0221

Total Coliform = 0.0022

### **Standard value recommended for each parameter (Si) -**
Dissolved oxygen(mg/L) = 10

pH = 8.5

Conductivity (µS/cm) = 1000

Biological oxygen demand (mg/L) = 5

Nitrate (mg/L) = 45

Fecal coliform/100 mL = 100

Total coliform/100 mL = 1000

### **Ideal value of each parameters (Videal) -**
Dissolved Oxygen = 14.6

pH = 7.0

Conductivity = 0

Biological Oxygen Demand = 0

Nitrate = 0

Fecal Coliform = 0

Total Coliform = 0
"""

# Calculating Water Quality Index of each sample
df_num_final = df_final.select_dtypes(exclude="object")
# Dropping year and Temp attribute because they are not used for computing WQI
df_num_final.drop(["year", "Temp"], axis=1, inplace=True)

# Weight Vector(wi)
wi = np.array([0.2213, 0.2604, 0.0022, 0.4426, 0.0492, 0.0221, 0.0022])

# Standard values of parameters(si)
si = np.array([10, 8.5, 1000, 5, 45, 100, 1000])

# Ideal values of paramters(vIdeal)
vIdeal = np.array([14.6, 7, 0, 0, 0, 0, 0])

def calc_wqi(sample):
    wqi_sample = 0
    num_col = 7
    for index in range(num_col):
        v_index = sample[index] # Obeserved value of sample at index
        v_index_ideal = vIdeal[index] # Ideal value of obeserved value
        w_index = wi[index] # weight of corresponding parameter of obeserved value
        std_index = si[index] # Standard value recommended for obeserved value
        q_index = (v_index) / (std_index)
        q_index = q_index * 100 # Final qi value of obeserved value
        wqi_sample += q_index*w_index
    return wqi_sample

# Computing WQI for the whole dataset
def calc_wqi_for_df(df):
    wqi_arr = []
    for index in range(df.shape[0]):
        index_row = df.iloc[index, :]
        wqi_row = calc_wqi(index_row)
        wqi_arr.append(wqi_row)
    return wqi_arr

wqi_arr = calc_wqi_for_df(df_num_final)

# Converting ordinary array to numpy array
wqi_arr = np.array(wqi_arr)
wqi_arr = np.reshape(wqi_arr, (-1, 1))

# Resetting index values of the dataframes
wqi_arr_df = pd.DataFrame(wqi_arr, columns=["WQI"]).reset_index()
df_final = df_final.reset_index()

# Combining dataframe of WQI and dataframe of attributes
df_wqi = pd.concat([df_final, pd.DataFrame(wqi_arr, columns=["WQI"])], axis=1)
df_wqi.drop("index", axis=1, inplace=True)
df_wqi.shape

# These are samples with negative WQI
df_wqi[(df_wqi["WQI"] < 0)]

# These are samples with WQI over 100
# df_wqi[(df_wqi["WQI"] > 100)]

# Removing the samples with negative WQI
df_pol_indices = df_wqi[(df_wqi["WQI"] > 100)].index
df_wqi.drop(df_pol_indices, axis=0, inplace=True)

"""      WQI Range                   Classification

     Less than 25                  Very Poor(0)
        26–50                         Poor(1)
        51-75                         Good(2)
    Greater than 75                 Excellent(3)

    https://www.omicsonline.org/open-access/assessment-of-water-quality-index-wqi-of-groundwater-in-rajkotdistrict-gujarat-india-2157-7617-1000341.php?aid=71110#:~:text=Based%20on%20the%20status%20of,is%20presented%20in%20Table%202.
"""

df_wqi["WQI clf"] = df_wqi["WQI"].apply(lambda x: (0 if (x <= 25)
                                        else(1 if (26<=x<=50)
                                        else(2 if (51<=x<=75)
                                        else 3))))

# df_wqi.tail()

# df_wqi.describe()

#pd.set_option('display.max_columns', None)
#pd.set_option('display.max_rows', None)
# df_wqi['WQI clf']

df_wqi["Is_Potable"] = df_wqi["WQI clf"].apply(lambda x: (1 if (x == 2 | x == 3)
                                        else(0)))

# df_wqi

#SAVING UPDATED DATAFRAME AS .csv FILE
df_wqi.to_csv('cleaned_data.csv')

df = pd.read_csv("cleaned_data.csv", encoding= 'unicode_escape')
# df.shape

df.isnull().sum().sort_values()

# df.describe()

colors = ['#BC5090','#FFA600']
print("HELLO?")
x= plt.figure(figsize=(4,4))
label = df_wqi['Is_Potable'].value_counts()
plt.pie(label.values,colors = colors, labels=label.index, autopct= '%1.1f%%', startangle=90)
plt.title('Is Potable?', fontsize=15)
st.pyplot(x)

# plt.show()
# st.pyplot(x)


df_wqi['Is_Potable'].value_counts()

sc_x = MinMaxScaler()
X = pd.DataFrame(sc_x.fit_transform(df_wqi.drop(["Temp", "Is_Potable", "STATION CODE",	"Address",
                                                 "Location", "WQI",	"WQI clf", "year"],axis = 1),),
                 columns=['DO','PH','Conductivity','BOD','NI','Fec_col','Tot_col'])

X

# KDE plots after removal of outliers
plot_kde(X.select_dtypes(exclude="object"))

# X.describe()

X.boxplot(figsize=(10,10))
# st.pyplot(X.boxplot(figsize=(10,10)))


#split data into Training and Test
X_train, X_test, y_train, y_test = train_test_split(X, df_wqi['Is_Potable'], test_size = 0.3, random_state = 100)
print(len(X_train))
print(len(X_test))
print(len(y_train))
print(len(y_test))

# import library

from imblearn.over_sampling import SMOTE
over = SMOTE()

# fit predictor and target variable
x_smote, y_smote = over.fit_resample(X_train, y_train)

print(x_smote.shape)
print(y_smote.shape)

from collections import Counter

print(Counter(y_smote))

"""# **Model Building**"""
print(x_smote)


SVM = svm.SVC()
SVM_pred = SVM.fit(x_smote, y_smote).predict(X_test)
SVM_Report = classification_report(y_test, SVM_pred, digits=4)
print(SVM_Report)

model = Sequential()
model.add(Embedding(7, 128))
model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(Dense(1))
model.add(Activation('sigmoid'))


model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_smote, y_smote, epochs=50, validation_data=(X_test, y_test))

y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5)

# Evaluation
from sklearn import metrics
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report

print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('R square:', r2_score(y_test, y_pred))

report = classification_report(y_test, y_pred, digits = 4)
print(report)

"""# **Optimised Models**

### Support Vector Machine
"""

from sklearn.model_selection import GridSearchCV
SVM_hyp2 = [{'kernel': ['rbf', 'linear', 'sigmoid'], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'C': [1, 10, 100, 1000]}]
grid = GridSearchCV(SVC(), SVM_hyp2, verbose = 3)
grid.fit(x_smote.values, y_smote.values)

print(grid.best_params_)

model_optimise = svm.SVC(C = 1000, gamma = 1, kernel = 'rbf', probability=True)
model_optimise.fit(x_smote.values, y_smote.values)

y_pred_optimised = model_optimise.predict(X_test)
print(classification_report(y_test, y_pred_optimised, digits=4))


#Save Optimised SVM Model
import pickle
pickle.dump(model_optimise, open('best_model.pkl', 'wb'))

#Model Testing
DO = 6.7
PH = 6.4
cond = 93
bod = 1.4
ni = 0.1
fec_col = 2147
tot_col = 3433


DO_trans = (DO - 2.5) / (10 - 2.5)
PH_trans = (PH - 0) / (9.01 - 0)
cond_trans = (cond - 11) / (18291 - 11)
bod_trans = (bod - 0.1) / (7.8 - 0.1)
ni_trans = (ni - 0) / (11 - 0)
fec_col_trans = (fec_col - 0) / (2367 - 0)
tot_col_trans = (tot_col - 0) / (	66382 - 0)

input = [[DO_trans, PH_trans, cond_trans, bod_trans, ni_trans, fec_col_trans, tot_col_trans]]
#output = scaler.fit_transform(input)
#print (output[0][0])
print(input)
test = model_optimise.fit(x_smote.values, y_smote.values)

output =  test.predict(input)
print(output[0])
